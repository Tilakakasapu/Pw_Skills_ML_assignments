{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c73232-461a-41f0-953e-4343b6877b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17857143],\n",
       "       [0.        ],\n",
       "       [0.53571429],\n",
       "       [1.        ],\n",
       "       [0.42857143]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min-Max scaling, also known as feature scaling or min-max normalization, is a data preprocessing technique used to transform numerical data so that it falls within a specified range, typically [0, 1]. \n",
    "#It's used to ensure that all features or variables in a dataset have similar scales, making them more compatible with machine learning algorithms that are sensitive to the magnitude of values.\n",
    "#supppose lets take you have score of students in a class so if you want to train a model you have to apply scaling to make the values fall in specific range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame({\"student marks\":[70,65,80,93,77]})\n",
    "scaler.fit_transform(df[['student marks']])#now you can see the data is transformed in to specific range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdb9ba7-0db7-480f-b44b-ae12767c9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Unit Vector technique in feature scaling, also known as vector normalization, is a method used to scale numerical data in such a way that the resulting vector (or feature) has a length of 1. It is often used in machine learning when the direction of the data points in a feature space is more important than their magnitude.\n",
    "#Suppose you're building a recommender system that recommends movies to users based on their preferences, and you represent user preferences as vectors in a high-dimensional space. \n",
    "#User A: [4, 3, 2, 0, 5] (ratings for Action, Comedy, Drama, Horror, and Romance genres)\n",
    "#User B: [5, 4, 0, 2, 3]\n",
    "#User C: [3, 2, 4, 1, 0]\n",
    "# sing the Unit Vector technique, we would normalize each user's preference vector to have a length of 1 while preserving the direction.\n",
    "# For User A ||A|| = 54**(1/2)\n",
    "# normalising user a : [(4/54**(1/2)),.. like that]\n",
    "# Min-Max Scaling: Scales data to a specific range, preserving the direction of preference in each dimension but not normalizing the overall length.\n",
    "\n",
    "# Unit Vector Scaling: Normalizes the vectors to have a length of 1, preserving the direction of preference while standardizing the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b52a4f0-b613-496f-baf3-a9082284843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in machine learning and data analysis. It's used to transform high-dimensional data into a lower-dimensional representation while preserving as much of the original variance or information as possible\n",
    "# for exaple lets take a two dimensional data like price of the house based on house size and no of rooms \n",
    "#so now when we plot a scatter plot of house size and no of rooms in x and y axis we get a 2d graph now taking a straight line having best fit and tring to mark these points on that line will make a 1 dimensional line but preserving the data on a single line it is called principal component analysis it is just an example\n",
    "# it will helps in reducing the dimension of the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c1d477-0421-4ab4-9429-f8c8c1387b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (Principal Component Analysis) and feature extraction are closely related concepts in the context of dimensionality reduction and data preprocessing. PCA can be used as a technique for feature extraction, where it transforms high-dimensional data into a lower-dimensional representation by identifying and retaining the most informative features or components.\n",
    "# Relationship between PCA and Feature Extraction:\n",
    "\n",
    "# Dimensionality Reduction: Both PCA and feature extraction aim to reduce the dimensionality of data. High-dimensional data can be computationally expensive to process and can suffer from the curse of dimensionality, where the data becomes sparse and noisy as the number of features increases.\n",
    "\n",
    "# Preserving Information: PCA and feature extraction techniques strive to retain as much relevant information as possible while reducing the number of features. They do this by identifying and capturing the underlying structure and patterns in the data.\n",
    "# Linear Combinations: PCA, in particular, works by finding linear combinations of the original features (principal components) that explain the maximum variance in the data. These linear combinations are often more informative than the original features themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d615788-f298-4096-822e-8b35e477c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataset contains features such as price, rating, and delivery time. These features likely have different scales, and Min-Max scaling can help make them more compatible for recommendation system modeling.\n",
    "#If the data is in similar scale then i wont applt standardisation if the data is in diffferent scale then i will apply standardiasation making the data having mean 0 and std deviation =1\n",
    "#Now i will apply minmaxscaling to price,rating,time as price may vary it have to be scaled to a specific range for training of the model and rating 0-5 is scaled to 0-1\n",
    "#after the scaling is done all the data is in range of 0-1 so now the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b75d5066-296a-431f-9982-6735dcfeb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Your Data: Start with your dataset containing various features related to stocks.\n",
    "# Standardize Data: Ensure all features have a similar scale by standardizing them (mean of 0, standard deviation of 1).\n",
    "# Apply PCA: Use PCA to identify the most important patterns in your data. PCA will create new, fewer features called \"principal components\" that summarize the information in the original features.\n",
    "# Choose Number of Components: Decide how many principal components to keep based on how much variance you want to retain (e.g., 95% of the total variance).\n",
    "# Transform Data: Project your stock data onto the selected principal components, reducing the number of features.\n",
    "# Build Your Model: Use the reduced dataset to build your stock price prediction model, which can be simpler and more efficient.\n",
    "# Evaluate and Fine-Tune: Assess your model's performance and make any necessary adjustments to improve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ffd335f-b5ae-4fe7-a213-b33d118f8f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'values': [1, 5, 10, 15, 20]})\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit_transform(df1[['values']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956e5dc-b2a4-47c3-a506-102d28ab33a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Standardize Data\n",
    "\n",
    "# Before applying PCA, it's important to standardize your data, especially when features are measured in different units. Standardization ensures that all features have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "# Step 2: Calculate Principal Components\n",
    "\n",
    "# Perform PCA on the standardized data to calculate the principal components. Each principal component represents a linear combination of the original features.\n",
    "\n",
    "# Step 3: Calculate Explained Variance Ratio\n",
    "\n",
    "# Determine the amount of variance explained by each principal component. You can calculate the explained variance ratio for each component, which tells you the proportion of the total variance in the data that is explained by that component.\n",
    "\n",
    "# Step 4: Decide on the Number of Components\n",
    "\n",
    "# Decide how many principal components to retain based on your goals. Here are some common strategies:\n",
    "\n",
    "# Preserve a Specific Amount of Variance: You can set a threshold for the cumulative explained variance ratio. For example, if you want to preserve 95% of the total variance, you retain enough principal components to reach or exceed this threshold.\n",
    "\n",
    "# Elbow Method: Plot the explained variance ratios and look for an \"elbow point\" where adding more components doesn't significantly increase the explained variance. This point can be a good choice for the number of components to retain.\n",
    "\n",
    "# Domain Knowledge: Sometimes, domain knowledge can guide your decision. For instance, if certain features are known to be highly significant, you may choose to retain them and a few additional components.\n",
    "\n",
    "# Model Performance: Consider the impact on your downstream modeling tasks. You can experiment with different numbers of components and assess how they affect the performance of your models (e.g., classification or regression).\n",
    "\n",
    "# Step 5: Retain the Chosen Components\n",
    "\n",
    "# Once you've decided on the number of components to retain, keep those principal components and discard the rest. These retained components will form the reduced feature set for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781f71d-e585-43a6-8e20-a69a539c802d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
