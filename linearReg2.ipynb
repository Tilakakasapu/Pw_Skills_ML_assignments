{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0f31f2-f865-4208-8eca-19a811eca551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #R SQUARE is a statistical method in ml to find the performance of the model it is used as performance metric in the MachineLeanring \n",
    "# it is actually calculated by the formla\n",
    "# RSquared = a - (sum of squared residual/sum of squared total)\n",
    "# here sum of squared residual represents sum of squares of difference between the actual values and the predicted values by the model\n",
    "# and sum of squared total represents the sum of squares of difference between the actual values and the maen value\n",
    "# In simple words the RSquare value tell how well the best fit line fits the data it will be 1 if the best fit line fits the data as the acutal and predicted values match\n",
    "# and it is near to zero if the best fit line does not fit properly and the datapoints are not equal to predicted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591510f2-7234-4ed4-8132-4221c8cc1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted r2 is also a statistical method used to find the goodness of fit in the model performance it is simiar to r square but it accounts the no of independent variables which is the limitation of r square\n",
    "# the differemce between the r square and the adjusted rsquare is that it rsquare increases if the no of independent variables increase nut the adjusted rsquare does not increase if no of independent variables increase\n",
    "#  takes into account the model's complexity by penalizing the inclusion of unnecessary predictors. It provides a more accurate reflection of how well the model fits the data, especially when you have multiple predictors.\n",
    "# Regular r2 tends to increase as you add more predictors, even if they do not contribute much to the model's performance. \n",
    "# In contrast, adjusted r2 will not increase if additional predictors do not improve the model's fit.\n",
    "# Adjusted r2 is always lower than or equal to regular r2.\n",
    "# It will be equal to r2 when the model has only one predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b7a099-92e0-461e-b990-91343e94b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted r2 is particularly useful when comparing models with different numbers of predictors. It helps you identify which model provides the best balance between explanatory power and complexity.\n",
    "# Researchers and data analysts often prefer adjusted r2 when evaluating models, as it offers a more conservative and accurate measure of model performance, especially in situations with many predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd4697b-5b28-40ec-ba09-4dc1ce852125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics to evaluate the performance of regression models. These metrics quantify the accuracy of the model's predictions by measuring the differences between predicted values and actual target values. Here's what they represent and how they are calculated:\n",
    "# Mean Absolute Error (MAE):\n",
    "# Definition: MAE measures the average absolute difference between the predicted values and the actual target values. It represents the average magnitude of errors.\n",
    "# Calculation: Calculate the absolute difference between each predicted value (yi) and the corresponding actual target value (ytrue), then take the average across all data points\n",
    "# Mean Squared Error (MSE):\n",
    "# Definition: MSE measures the average squared difference between predicted values and actual target values. It gives more weight to larger errors, making it sensitive to outliers.\n",
    "# Calculation: Calculate the squared difference between each predicted value (yi) and the corresponding actual target value ytrue  then take the average across all data points\n",
    "# Root Mean Squared Error (RMSE):\n",
    "# Definition: RMSE is the square root of MSE. It provides a measure of the average magnitude of the errors in the same units as the target variable. RMSE is more interpretable than MSE.\n",
    "# Calculation: Calculate MSE as described above and then take the square root:\n",
    "# RMSE = (MSE)^1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf1a466-5b6f-4801-824f-3481ad012001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages:\n",
    "# RMSE (Root Mean Squared Error):\n",
    "# Advantage: RMSE is sensitive to large errors due to its squaring effect. It gives more weight to significant errors, which can be useful when large errors are particularly undesirable.\n",
    "# Use Case: It's a good choice when you want to penalize larger errors more heavily, such as in cases where outliers need to be addressed.\n",
    "# MSE (Mean Squared Error):\n",
    "# Advantage: MSE provides a smooth and differentiable loss function that is suitable for optimization algorithms, making it easier to use in model training.\n",
    "# Use Case: It's commonly used in optimization algorithms, including gradient descent.\n",
    "# MAE (Mean Absolute Error):\n",
    "# Advantage: MAE is more robust to outliers because it uses absolute differences, making it less sensitive to extreme values.\n",
    "# Use Case: When you want a metric that gives equal importance to all errors, MAE is a good choice.\n",
    "# Disadvantages:\n",
    "# RMSE (Root Mean Squared Error):\n",
    "# Disadvantage: RMSE can be heavily influenced by outliers, which may not always be desirable, especially if outliers are part of the natural data distribution.\n",
    "# Interpretation: It may be less intuitive to interpret because it's in the square root of the units of the target variable.\n",
    "# MSE (Mean Squared Error):\n",
    "# Disadvantage: MSE magnifies the impact of outliers due to squaring, which can lead to a distorted view of model performance.\n",
    "# Interpretation: Like RMSE, interpreting MSE can be challenging due to the squared units.\n",
    "# MAE (Mean Absolute Error):\n",
    "# Disadvantage: MAE may not adequately penalize large errors, which could be a drawback in situations where significant errors need to be minimized.\n",
    "# Optimization: It may not be as well-suited for optimization problems that rely on gradient-based methods due to the lack of differentiability at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3e1060-0c4d-4cd0-b4d7-0db45d3bd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # asso Regularization (Least Absolute Shrinkage and Selection Operator) is a technique used in linear regression to add a penalty term to the linear regression cost function. It encourages some of the model's coefficients to be exactly zero, effectively performing feature selection and simplifying the model. Here's how it differs from Ridge Regularization:\n",
    "# # Effect on Coefficients: Lasso tends to force some coefficients to be exactly zero, effectively performing feature selection. It retains a subset of the most influential predictors while setting others to zero.\n",
    "# # Geometric Interpretation: In geometric terms, Lasso regularization creates a diamond-shaped constraint region, leading to intersection with the axes. This encourages sparsity in the coefficient values.\n",
    "# Lasso: Use Lasso when you suspect that many of your predictors are irrelevant or redundant, and you want a sparse model with feature selection. Lasso can be helpful when you want to identify and retain the most important features in your dataset.\n",
    "\n",
    "# Ridge: Use Ridge when you want to reduce the impact of multicollinearity (high correlation among predictors) and control for overfitting. Ridge tends to work well when all predictors are relevant, and you don't want to exclude any from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6469bb2-42fa-4b78-9045-bb3e318da90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized linear models, such as Ridge and Lasso regression, help prevent overfitting by adding penalty terms to the linear regression cost function. These penalty terms constrain the model's coefficients, preventing them from becoming too large or complex. Here's how regularized linear models prevent overfitting:\n",
    "#     Consider a scenario where you have a dataset with many predictors, some of which may not be relevant to the target variable. Without regularization, a standard linear regression model might overfit the training data by assigning large coefficients to all predictors. This results in a complex model that fits noise and performs poorly on unseen data.\n",
    "\n",
    "# By applying Ridge or Lasso regularization, the model is constrained from assigning excessively large coefficients to less important predictors. As a result, the model becomes simpler, more interpretable, and less prone to overfitting. It can generalize better to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "750d765b-77f3-482b-a692-c3194e89e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized linear models, such as Ridge and Lasso regression, are powerful techniques, but they have limitations and may not always be the best choice for regression analysis. Here are some limitations:\n",
    "\n",
    "# Loss of Information: Regularization techniques can shrink coefficients, leading to a loss of information if all predictors are relevant. In cases where all predictors are important, a regularized model may underfit the data.\n",
    "\n",
    "# Not Suitable for All Problems: Regularization may not be appropriate for all types of problems. For instance, when dealing with nonlinear relationships, tree-based models like Random Forests or nonlinear models like neural networks may be more suitable.\n",
    "\n",
    "# Interpretability: Regularized models like Ridge and Lasso can make the model less interpretable, especially when some coefficients are shrunk to zero. This can be a drawback in situations where interpretability is crucial.\n",
    "\n",
    "# Optimal Hyperparameter Tuning: Choosing the right regularization strength  can be challenging. Tuning this hyperparameter requires careful consideration, and the choice may vary depending on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000b35e7-aed5-4ced-ab8a-c5e449e36101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A (RMSE = 10):\n",
    "\n",
    "# RMSE is more sensitive to larger errors, so Model A might be penalizing significant errors more heavily.\n",
    "# If minimizing large errors is critical for your application (e.g., in financial modeling or safety-critical systems), Model A might be preferred.\n",
    "# Model B (MAE = 8):\n",
    "\n",
    "# MAE gives equal weight to all errors, which may be more suitable if all errors are considered equally important.\n",
    "# If interpretability and ease of understanding the average magnitude of errors are essential, Model B might be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7154c11c-663d-4597-9c3c-38d3735fcbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A (Ridge Regularization with aplha =0.1) \n",
    "# Ridge regularization tends to work well when you have a dataset with multicollinearity (high correlation among predictors) because it can help control multicollinearity by shrinking the coefficients.\n",
    "# Model B (Lasso Regularization with alpha =0.5\n",
    "# Lasso regularization is effective for feature selection because it tends to force some coefficients to be exactly zero, effectively removing irrelevant predictors.\n",
    "#          0.5, the Lasso penalty is moderate, which means it may perform feature selection while allowing for the inclusion of some predictors.\n",
    "#          f interpretability and feature selection are essential, Model B (Lasso) might be preferred, as it can automatically identify and exclude less relevant predictors.\n",
    "# If you suspect multicollinearity is a significant issue in your dataset, Model A (Ridge) may be preferred, as it effectively reduces multicollinearity without excluding predictors entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9bbcb-f9bc-4314-954e-9255125d2328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
