{
 "cells": [
  {
   "cell_type": "raw",
   "id": "243a5835-de80-4b7b-b151-86d8106bb595",
   "metadata": {},
   "source": [
    "OVER FITTING AND UNDER FITTING IN MACHINE LEARNING:\n",
    "\n",
    "OVETFITTING: OVERFITTING IS THE CONDITION IN WHICH THE MODEL THAT IS TRAINED ON THE DATASET PERFORMS EXCELLENTLY IN TRAINING DATA BUT WHENEVER IT IS GIVEN A NEW DATA IT WILL NOT PERFORM THAT MUCH EFFICIENTLY.\n",
    "Overfitting:\n",
    "\n",
    "High Training Accuracy, Low Testing Accuracy: Overfit models perform exceptionally well on the training data but poorly on unseen or testing data. This happens because the model has learned the noise in the training data and has essentially memorized it.\n",
    "\n",
    "Poor Generalization: Overfit models fail to generalize to new, unseen data. They are too tailored to the training data and do not capture the underlying patterns that are applicable to other data points.\n",
    "\n",
    "UNDER FITTING: UNDER FITTING IS THE CONDITION IN WHICH THE MODEL PERFORMANCE IS LOW OON BOTH TEST AND TRAINING DATA. IT OCCURS BECAUSE MODEL IS TOO SIMPLE TO CAPTURE UNDERLYING STRUCTURE IN THE DATA\n",
    "\n",
    "Low Training Accuracy and Low Testing Accuracy: Underfit models do not perform well on either the training data or the testing data. They are too simple to capture the underlying patterns in the data.\n",
    "\n",
    "Failure to Capture Relationships: Underfit models fail to capture the true relationships between features and the target variable. They may miss important patterns or trends in the data.\n",
    "\n",
    "Bias: Underfit models are often biased and may make incorrect assumptions about the data, leading to systematic errors in predictions.\n",
    "\n",
    "Inadequate Representations: In the context of deep learning, underfitting may result from shallow or insufficiently complex neural networks that cannot capture the complexity of the data.\n",
    "\n",
    "Limited Usefulness: Underfit models have limited usefulness in real-world applications because they do not provide accurate predictions or insights into the data.\n",
    "\n",
    "Mitigating Overfitting:\n",
    "Reduce Model Complexity: Use simpler models or reduce the complexity of existing ones.\n",
    "Feature Selection: Choose relevant features and remove noisy ones.\n",
    "Regularization: Apply L1 or L2 regularization to penalize large coefficients.\n",
    "Cross-Validation: Use cross-validation to evaluate and tune the model.\n",
    "Early Stopping: Stop training when performance on a validation set degrades.\n",
    "\n",
    "Mitigating Underfitting:\n",
    "Increase Model Complexity: Use more complex models with more parameters.\n",
    "Feature Engineering: Create informative features and transform existing ones.\n",
    "Hyperparameter Tuning: Adjust hyperparameters to better fit the data.\n",
    "Ensemble Methods: Combine multiple models for increased complexity.\n",
    "Collect More Data: Gather additional data to capture underlying patterns.\n",
    "Model Interpretability: Use interpretable models to understand data relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6612361-fdf9-4a55-9cdd-64375feee6d4",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "#### To reduce overfitting, you can:\n",
    "#### Use simpler models with fewer parameters.\n",
    "#### Increase the amount of training data.\n",
    "#### Apply regularization techniques like L1 or L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d5795-7970-45f9-8996-baeb39f7a1ac",
   "metadata": {},
   "source": [
    "Q3\n",
    "#### Underfitting occurs when a model is too simple to capture the underlying patterns in the data. It will perform badly at both training data and validation data\n",
    "##### It can happen in scenarios like:\n",
    "#### Using a linear model for a highly nonlinear problem.\n",
    "#### Not having enough relevant features in the dataset.\n",
    "#### Setting overly strict constraints on a model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cc09be4-bc73-4039-bbc2-280dfcf6635d",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in ML.Bias represents the error due to overly simplistic assumptions in the learning algorithm, leading to underfitting. Variance represents the error due to too much complexity in the algorithm, causing overfitting. There's an inverse relationship between bias and variance: as you reduce one, the other increases.\n",
    "when the model performs too well in the training but badly at test data it is called overfitting.\n",
    "when the model performs badly at both training and test data it is called under fitting.\n",
    "Finding the right balance between them is essential for achieving good model performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0525db74-ea60-4f90-8379-d62f142d53a4",
   "metadata": {},
   "source": [
    "Q5\n",
    "Common methods for detecting overfitting and underfitting include:\n",
    "Using cross-validation to assess model performance on multiple subsets of the data.\n",
    "Analyzing the model's behavior on a holdout validation dataset.\n",
    "Examining the model's training and test error rates; a significant gap suggests overfitting, while high error on both suggests underfitting.\n",
    "by observing the model performance in training data and test data we can conclude the model is underfitting or over fitting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "256495df-1a09-46f7-9e08-9f1ce3b73890",
   "metadata": {},
   "source": [
    "Q6\n",
    "Bias vs. Variance Trade-off: Bias and variance represent opposite ends of a trade-off in machine learning. Increasing model complexity reduces bias but increases variance, and vice versa. The goal is to find the right balance to achieve good generalization.\n",
    "\n",
    "Performance:\n",
    "High bias models typically have poor performance on both the training and testing datasets because they cannot capture the underlying patterns. They consistently make errors.\n",
    "High variance models may perform well on the training data but poorly on the testing data due to their sensitivity to noise. They have trouble generalizing to new, unseen data.\n",
    "Examples:\n",
    "\n",
    "High Bias Model Example:\n",
    "\n",
    "Linear Regression: A linear regression model assumes a linear relationship between features and the target variable. If the true relationship is nonlinear, the model will have high bias and underfit the data. It consistently predicts values that are far from the actual values.\n",
    "High Variance Model Example:\n",
    "\n",
    "Deep Neural Network with No Regularization: A deep neural network with many layers and no regularization techniques (like dropout or weight decay) can have high variance. It can memorize the training data, fitting even the noise, and perform poorly on new data due to overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "14f76cd4-9041-4bc1-8712-d10c5ba544db",
   "metadata": {},
   "source": [
    "Q7\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function, discouraging large parameter values. Common regularization techniques include:\n",
    "\n",
    "L1 Regularization (Lasso): Adds the absolute value of parameter weights to the loss function. Encourages sparse models with some parameters set to zero.\n",
    "\n",
    "L2 Regularization (Ridge): Adds the squared value of parameter weights to the loss function. Encourages smaller parameter values.\n",
    "\n",
    "ELASTIC NET REGULARISATION : It combines of all both l1 and l2 regularisation and called elastic regularisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05121ec2-3bf2-47f1-b650-c91a0e780276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
