{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850b13d-00c5-44db-aed3-56650b32032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.\n",
    "# Linear Regression is used for regression tasks, where the goal is to predict a continuous numerical output. It models the relationship between the independent variables nd the dependent variable as a linear equation.\n",
    "# Logistic Regression, on the other hand, is used for classification tasks, where the goal is to predict discrete categorical outcomes, typically binary (e.g., yes/no, 0/1). It models the probability of an instance belonging to a particular class.\n",
    "# Example scenario for Logistic Regression:\n",
    "# Suppose you want to predict whether an email is spam (1) or not spam (0) based on various features of the email, such as the sender, subject, and content. Since this is a binary classification task, logistic regression is more appropriate to model the probability of an email being spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e41d1c2-5603-4742-a972-3c38ef84f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cost function used in logistic regression is the Logistic Loss, also known as the Log Loss.\n",
    "# cost(y,yi) = -ylog(yi)+(1-y)(log(1-yi))\n",
    "# y is the true class label (0 or 1).\n",
    "# yi is the predicted probability of the instance belonging to class 1\n",
    "# The goal in logistic regression is to minimize this cost function.Optimization techniques like Gradient Descent or Newton's \n",
    "# method are commonly used to find the model parameters that minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415437fa-1ad0-4b7d-91a3-ec59374e1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization in logistic regression is used to prevent overfitting by adding a penalty term to the cost function. The two common types of regularization used are L1 regularization and L2 regularization:\n",
    "# L1 Regularization (Lasso): It adds the absolute values of the coefficients to the cost function. This encourages some coefficients to be exactly zero, effectively performing feature selection.\n",
    "# L2 Regularization (Ridge): It adds the squares of the coefficients to the cost function. It reduces the magnitude of coefficients and discourages them from becoming too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2dfa57-99c6-4d86-bf55-28b86ed09469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n",
    "# The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classification model, like logistic regression, at various classification thresholds. It plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) at different threshold settings.\n",
    "# The ROC curve helps evaluate a model's ability to distinguish between the positive and negative classes. The area under the ROC curve (AUC-ROC) is a commonly used metric to quantify the model's performance. A higher AUC-ROC indicates better discrimination between classes, with a perfect classifier having an AUC-ROC of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d3eaeb-e93a-4d9f-8b23-04fea7966a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?\n",
    "\n",
    "# Common techniques for feature selection in logistic regression include:\n",
    "\n",
    "# L1 Regularization (Lasso): By adding L1 regularization, logistic regression can automatically perform feature selection by setting some coefficients to exactly zero. This eliminates less important features.\n",
    "\n",
    "# Recursive Feature Elimination (RFE): RFE iteratively removes the least important features and trains the model until the desired number of features is reached. It ranks features by their impact on model performance.\n",
    "\n",
    "# Feature Importance from Tree-Based Models: Tree-based models like Random Forest or Gradient Boosting can provide feature importance scores. Features with low importance scores can be excluded from logistic regression.\n",
    "\n",
    "# Correlation Analysis: Identifying and removing highly correlated features can help eliminate redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e879521-5982-40cf-83e4-b38e5c1cfbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling:\n",
    "\n",
    "# Oversampling: Increase the number of instances in the minority class by duplicating or generating synthetic examples.\n",
    "# Undersampling: Reduce the number of instances in the majority class by randomly removing examples.\n",
    "# Changing the Threshold: Adjust the classification threshold to favor sensitivity (recall) over specificity, depending on the problem's priorities.\n",
    "\n",
    "# Cost-Sensitive Learning: Assign different misclassification costs to different classes to make the model more sensitive to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20772c-921f-4bde-b171-7b096924de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity: Multicollinearity occurs when independent variables are highly correlated. To address this, you can:\n",
    "\n",
    "# Remove one of the highly correlated variables.\n",
    "# Apply regularization techniques (e.g., Ridge or Elastic Net) to shrink coefficients and mitigate multicollinearity.\n",
    "# Use dimensionality reduction techniques like Principal Component Analysis (PCA) to create orthogonal variables.\n",
    "# Imbalanced Data: For imbalanced datasets, consider resampling techniques, threshold adjustment, cost-sensitive learning, or ensemble methods to handle class imbalance.\n",
    "\n",
    "# Outliers: Identify and handle outliers in the dataset using methods like winsorization, transformation, or robust regression techniques.\n",
    "\n",
    "# Non-linearity: If the relationship between predictors and the target is non-linear, consider feature engineering to create polynomial features or using non-linear models like decision trees or support vector machines.\n",
    "\n",
    "# Missing Data: Deal with missing values by imputation or using models that handle missing data well, like decision trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
