{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd664ab-dc6d-4555-9075-3009868df39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression:\n",
    "\n",
    "# Definition: Simple linear regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (target) by fitting a linear equation to the observed data.\n",
    "# Equation: In simple linear regression, the equation of the line is represented as: y = mx + b, where y is the dependent variable, x is the independent variable, m is the slope of the line, and b is the y-intercept.\n",
    "# Example: Let's say you want to predict a person's weight (y) based on their height in inches (x). You collect data on several individuals' heights and weights and use simple linear regression to find a line that best fits the relationship between height and weight.\n",
    "# Multiple Linear Regression:\n",
    "\n",
    "# Definition: Multiple linear regression extends simple linear regression to model the relationship between a dependent variable and two or more independent variables by fitting a linear equation to the observed data.\n",
    "# Equation: In multiple linear regression, the equation is: y = b₀ + b₁x₁ + b₂x₂ + ... + bₖxₖ, where y is the dependent variable, x₁, x₂, ..., xₖ are the independent variables, and b₀, b₁, b₂, ..., bₖ are the coefficients.\n",
    "# Example: Suppose you want to predict a house's sale price (y) based on various factors like the number of bedrooms (x₁), square footage (x₂), and neighborhood quality (x₃). You collect data on multiple houses, including these features, and use multiple linear regression to model the relationship between these factors and the house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fee4da6-9ae8-4c7b-be5e-e06c54133039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearity: The relationship between the independent variables (predictors) and the dependent variable (target) is assumed to be linear.\n",
    "# No or Little Outliers: Outliers are data points that significantly deviate from the general trend.\n",
    "# No Perfect Linear Relationships: There should be no perfect linear relationships between the independent variables. You can check this by examining correlation coefficients and conducting hypothesis tests to identify problematic variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d70c978-2083-487d-be5f-1bbb650602b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Slope (Coefficient of the Independent Variable): The slope represents the change in the dependent variable for a one-unit change in the independent variable, assuming all other variables are held constant. In other words, it quantifies the rate of change in the dependent variable for each unit change in the independent variable.\n",
    "# # # Intercept: The intercept represents the value of the dependent variable when all independent variables are set to zero. It is the value where the regression line crosses the y-axis. In many cases, the intercept may not have a meaningful interpretation, especially if setting all independent variables to zero is not a realistic scenario.\n",
    "# Suppose you want to predict house prices based on the size (in square feet) of the house as the independent variable. You collect data on various houses, their sizes, and their corresponding prices, and you fit a linear regression model to the data.\n",
    "#  Let's say the intercept is $50,000. This means that if the size of a house were zero square feet (which is an unrealistic scenario), the predicted price of that house would still be $50,000. In this context, the intercept may not have a meaningful interpretation, as a house with zero square feet doesn't exist.\n",
    "# : Let's say the slope is 200. This means that for each additional square foot in house size, the predicted house price increases by $200. So, if a house is 1,000 square feet, you can predict its price to be $50,000 (intercept) + $200 (slope) * 1,000 square feet = $250,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113fbf3a-2cc1-43b3-b464-bdc8902e9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gradient descent is an optimization algorithm used in machine learning to minimize a cost function or loss function. It is a fundamental technique in training various machine learning models, particularly those that involve finding the optimal values for a set of parameters. Gradient descent is employed to iteratively adjust these parameters in order to minimize the cost function, which represents the error or discrepancy between the model's predictions and the actual target values.\n",
    "# Model Initialization: At the beginning of the training process, the model's parameters are typically initialized with random values. These parameters determine how the model makes predictions.\n",
    "\n",
    "# Calculating the Error: The model makes predictions using these initial parameters, and the error or loss is calculated by comparing these predictions to the actual target values. The goal is to minimize this error.\n",
    "\n",
    "# Gradient Calculation: Gradient descent computes the gradient of the cost or loss function with respect to each model parameter. The gradient indicates the direction and magnitude of the steepest increase in the error. This is done by taking the derivative of the loss function with respect to each parameter.\n",
    "\n",
    "# Parameter Update: Using the gradients, gradient descent updates the model parameters by taking a step in the opposite direction of the gradient. The size of the step is controlled by a hyperparameter called the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb8c842-76e6-40b9-9814-1cff7d4295f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity is a common issue in multiple linear regression when two or more independent variables in a model are highly correlated with each other. This high correlation makes it challenging to determine the individual effects of each variable on the dependent variable, as it becomes difficult to distinguish their unique contributions. Multicollinearity can create problems in regression analysis, including unstable coefficient estimates and difficulty in interpreting the model.\n",
    "# Remove or Combine Variables: Consider removing one or more of the highly correlated variables from the model. If two variables are conceptually similar, keeping one and discarding the other may help. Alternatively, you can create a composite variable by combining related variables.\n",
    "# Feature Selection: Use feature selection techniques like backward elimination, forward selection, or stepwise regression to automatically select a subset of the most relevant independent variables while excluding the highly correlated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3721a0-8ad0-4919-a294-50b392555181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression is a variation of linear regression that allows you to model relationships between variables that are not linear but instead follow a polynomial curve. In linear regression, the relationship between the independent variable(s) and the dependent variable is assumed to be a straight line, while in polynomial regression, this relationship can take on a curved shape.\n",
    "\n",
    "# Here's how polynomial regression differs from linear regression:\n",
    "\n",
    "# Linearity vs. Non-Linearity: In linear regression, the relationship between the independent variable(s) and the dependent variable is linear, represented by a straight line. In polynomial regression, the relationship can take a non-linear form, such as a curve\n",
    "# Flexibility: Polynomial regression is more flexible in capturing complex relationships in data, including curves, peaks, and valleys. Linear regression can only model linear relationships.\n",
    "\n",
    "# Degree of the Polynomial: You can choose the degree of the polynomial, which determines how curvy the relationship can be. A higher degree allows for more complex curves but also increases the risk of overfitting (fitting noise in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7041e-31d1-4116-bba2-3fac530ae553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages of Polynomial Regression:\n",
    "\n",
    "# Flexibility: Polynomial regression can model complex, non-linear relationships in the data, allowing for a better fit when the relationship is not linear.\n",
    "\n",
    "# Improved Fit: It can provide a better fit to the data compared to linear regression, particularly when the true relationship has curves or bends.\n",
    "\n",
    "# Captures Local Patterns: Polynomial regression can capture local variations and patterns in the data that linear regression may miss.\n",
    "\n",
    "# Disadvantages of Polynomial Regression:\n",
    "\n",
    "# Overfitting: As the degree of the polynomial increases, the model can become too complex and may overfit the training data, capturing noise instead of the underlying trend.\n",
    "\n",
    "# Interpretability: Higher-degree polynomial models can be challenging to interpret, making it difficult to explain the relationship to non-technical stakeholders.\n",
    "\n",
    "# Extrapolation Issues: Polynomial models can produce unrealistic results when extrapolating beyond the range of the training data.\n",
    "# Use polynomial regression when:\n",
    "\n",
    "# There is strong evidence that the relationship between the variables is non-linear.\n",
    "\n",
    "# You have domain knowledge suggesting a particular degree of polynomial may be appropriate.\n",
    "\n",
    "# You are willing to carefully validate the model to avoid overfitting, possibly using techniques like cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604258c-d3fc-45cb-925e-ce87ed08a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f32f28-4abd-419b-9f47-4ff6fee4e0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
