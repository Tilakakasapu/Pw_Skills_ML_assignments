{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yt6dKsn8T1mE"
      },
      "outputs": [],
      "source": [
        "# Weight initialization is a critical step in training artificial neural networks (ANNs) as it can significantly impact the convergence and performance of the model.\n",
        "# Proper weight initialization helps in overcoming challenges associated with convergence and ensures that the model learns effectively.\n",
        "# Why is it Necessary to Initialize Weights Carefully:\n",
        "# Avoiding Vanishing or Exploding Gradients\n",
        "# symmetry breaking\n",
        "# faster convergence\n",
        "\n",
        "# Challenges Associated with Improper Weight Initialization:\n",
        "# explaoiding gradients : with out the proper weight initaialisation the vanishing gradient problems may occur in the network and the model cannot learn efficiently\n",
        "# divergence : when weights are not properly initialised the model may fail to converge to the optimal soution\n",
        "# Slow Convergence:If weights are not initialized properly, the network may converge very slowly or get stuck in a local minimum, making it difficult to train effectively.\n",
        "# Symmetry Issues:Poor weight initialization can result in symmetry issues, where neurons in the same layer learn the same features, limiting the expressive power of the network.\n",
        "\n",
        "# Variance in weight initialization is crucial because it affects the scale of activations in the network.\n",
        "#  Activations that are too small or too large can lead to the aforementioned issues such as vanishing/exploding gradients and slow convergence.\n",
        "# The variance of weights should be carefully chosen based on the architecture of the network and the activation functions used.\n",
        "#  For example, Xavier/Glorot initialization and He initialization are methods that consider the variance of weights based on the number of input and output units in a layer, helping to maintain a balance during training.\n",
        "# careful weight initialization is essential to overcome convergence challenges, avoid issues like vanishing or exploding gradients, and promote stable and effective learning in neural networks. Variance plays a crucial role in determining the scale of weights, and choosing appropriate initialization methods helps in achieving better training outcomes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  When all the weights in a neural network are initialized to zero, the derivative with respect to the loss function is the same for every weight\n",
        "# in the network's weight matrix. As a result, all the weights take on the same value in subsequent iterations, making the hidden layers symmetric. This process\n",
        "# continues for all the iterations, resulting in a neural network that is no better than a linear model. It is important to note that setting biases to zero does\n",
        "# #  not cause any issues since non-zero weights break the symmetry, and even if the bias is zero, the values in each neuron will still be different.\n",
        "# While zero initialization is straightforward and easy to implement, it comes with several limitations that make it less desirable for many scenarios:\n",
        "# Symmetry Issues:\n",
        "# All neurons in a layer initialized to zero will learn the same features during training. This results in symmetry issues, where neurons in the same layer are updated identically, limiting the expressive power of the network.\n",
        "# Vanishing Gradients:\n",
        "# During backpropagation, if all weights are initialized to zero, the gradients for each weight will be the same. This leads to vanishing gradients, where the gradients become extremely small, causing slow or stalled learning, especially in deep networks.\n",
        "# Lack of Capacity to Break Symmetry:\n",
        "# Neurons need to have different initial weights to break symmetry and allow each neuron to learn different features. Zero initialization fails to provide this diversity.\n",
        "# When Zero Initialization can be Appropriate:\n",
        "# Linear Activation Functions:\n",
        "# In networks where the activation function is strictly linear (e.g., identity function), zero initialization may not lead to symmetry issues or vanishing gradients. However, such networks may not effectively capture non-linear relationships in the data."
      ],
      "metadata": {
        "id": "HStdT9bpUv3q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Random weight initialization is a technique used to initialize the weights of neural network connections with random values in a specific range. The idea behind this technique is to break the symmetry of weights and prevent vanishing or exploding gradients during the training process.\n",
        "# The most commonly used method for random weight initialization is to generate weights from a normal distribution with a mean of 0 and a standard deviation of 1. This means that the weights are randomly initialized with values centred around 0 and spread out within a certain range.\n",
        "# However, this method can result in weights that are too small or too large, which can slow down the learning process or cause numerical instability. Therefore, a more commonly used approach is to scale the randomly generated weights by a factor that depends on the number of input and output connections for each neuron."
      ],
      "metadata": {
        "id": "hB2r6jnGWqvl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Xavier weight initialization, also known as Glorot initialization, is a technique for initializing the weights of a neural network. The objective of this technique is to prevent the vanishing or exploding gradient problem during the training of the network. The idea behind Xavier weight initialization is to set the initial weights in such a way that the variance of the outputs of each neuron is the same as the variance of its inputs. This ensures that the gradients do not vanish or explode as they propagate through the network during backpropagation.\n",
        "# - The formula for Xavier weight initialization for a layer with n inputs and m outputs is:\n",
        "# python\n",
        "# W = np.random.randn(n, m) * np.sqrt(1/n)\n",
        "# where W is the weight matrix for the layer, np.random.randn(n, m) generates a matrix of random numbers with a normal distribution, and np.sqrt(1/n) scales the random numbers to ensure that the variance of the outputs of each neuron is the same as the variance of its inputs.\n",
        "# - Here, the factor 1/n is used because we want the variance of the outputs to be proportional to the number of inputs. This ensures that the variance of the gradients with respect to the inputs is roughly the same for each layer, which helps prevent the gradients from vanishing or exploding.\n",
        "# - Let's consider an example to understand Xavier weight initialization better. Suppose we have a neural network with an input layer of size 1000, a hidden layer of size 500, and an output layer of size 10. We can initialize the weights of the hidden layer using Xavier weight initialization as follows:\n",
        "# python\n",
        "# W1 = np.random.randn(1000, 500) * np.sqrt(1/1000)\n",
        "# W2 = np.random.randn(500, 10) * np.sqrt(1/500)\n",
        "# ```\n",
        "# - Here, W1 is the weight matrix for the hidden layer with 1000 inputs and 500 outputs, and W2 is the weight matrix for the output layer with 500 inputs and 10 outputs. The np.sqrt(1/n) term ensures that the variance of the outputs of each neuron is the same as the variance of its inputs.\n",
        "# - By using Xavier weight initialization, we can ensure that the network trains faster and achieves better accuracy compared to random weight initialization or zero weight initialization."
      ],
      "metadata": {
        "id": "zeyeh7mTW1Rt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  He Weight Initialization is a weight initialization technique used in neural networks. It is an improvement over the Xavier initialization method and is commonly used in deep neural networks that use the ReLU activation function.\n",
        "\n",
        "# - The basic idea behind He initialization is to initialize the weights of each neuron in the network with random values drawn from a Gaussian distribution with a mean of 0 and a standard deviation of sqrt(2/n), where n is the number of inputs to the neuron.\n",
        "\n",
        "# - The formula for He initialization is given as:\n",
        "\n",
        "# W ~ N(0, sqrt(2/n))\n",
        "\n",
        "# Where,\n",
        "# W - weight matrix\n",
        "# N - normal distribution\n",
        "# 0 - mean\n",
        "# sqrt(2/n) - standard deviation\n",
        "\n",
        "# - The factor of sqrt(2/n) in the standard deviation helps to maintain a balance between the variance of the activations and the variance of the gradients in the network, preventing vanishing or exploding gradients.\n",
        "\n",
        "# - He initialization is effective for networks that use the ReLU activation function, as it helps to address the problem of vanishing gradients that can occur when using a small initial weight range with ReLU.\n"
      ],
      "metadata": {
        "id": "mkSWz_lmXhba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "SfM7S2Eqacu8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train_full,y_train_full),(x_test,y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJrtypOiacrf",
        "outputId": "c1eb6f3b-99e5-4377-ff7f-2538e197802c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid,x_train = x_train_full[:5000]/255.,x_train_full[5000:]/255.\n",
        "y_valid,y_train = y_train_full[:5000],y_train_full[5000:]\n",
        "x_test = x_test/255."
      ],
      "metadata": {
        "id": "eCEpehkxacos"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [tf.keras.layers.Flatten(input_shape=[28,28],name=\"inputlayer\"),\n",
        "          tf.keras.layers.Dense(300,activation = \"relu\",kernel_initializer=tf.keras.initializers.HeNormal(seed = None),name = \"hiddenlayer\"),\n",
        "          tf.keras.layers.Dense(100,activation = \"relu\" , kernel_initializer=tf.keras.initializers.HeNormal(seed = None),name = \"hiddenlayer2\"),\n",
        "          tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
        "model = tf.keras.models.Sequential(layers)"
      ],
      "metadata": {
        "id": "OwPtqmsxacl6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRRK3xK9acjI",
        "outputId": "320a7de7-3d27-4f62-d260-2203c5d0feeb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inputlayer (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " hiddenlayer (Dense)         (None, 300)               235500    \n",
            "                                                                 \n",
            " hiddenlayer2 (Dense)        (None, 100)               30100     \n",
            "                                                                 \n",
            " outputLayer (Dense)         (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 266610 (1.02 MB)\n",
            "Trainable params: 266610 (1.02 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layers2 = [tf.keras.layers.Flatten(input_shape=[28,28],name=\"inputlayer\"),\n",
        "          tf.keras.layers.Dense(300,activation = \"relu\",kernel_initializer=tf.keras.initializers.GlorotUniform(seed = None),name = \"hiddenlayer\"),\n",
        "          tf.keras.layers.Dense(100,activation = \"relu\" , kernel_initializer=tf.keras.initializers.GlorotUniform(seed = None),name = \"hiddenlayer2\"),\n",
        "          tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
        "model2 = tf.keras.models.Sequential(layers2)"
      ],
      "metadata": {
        "id": "f-7HOBsZacgS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers3 = [tf.keras.layers.Flatten(input_shape=[28,28],name=\"inputlayer\"),\n",
        "          tf.keras.layers.Dense(300,activation = \"relu\",kernel_initializer=tf.keras.initializers.GlorotNormal(seed = None),name = \"hiddenlayer\"),\n",
        "          tf.keras.layers.Dense(100,activation = \"relu\" , kernel_initializer=tf.keras.initializers.GlorotNormal(seed = None),name = \"hiddenlayer2\"),\n",
        "          tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
        "model3 = tf.keras.models.Sequential(layers3)"
      ],
      "metadata": {
        "id": "vqKOaiXFacOO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = \"sparse_categorical_crossentropy\"\n",
        "optimizer = \"SGD\"\n",
        "Metrics = [\"accuracy\"]\n",
        "model.compile(loss = loss,optimizer = optimizer,metrics = Metrics)\n",
        "model2.compile(loss = loss,optimizer = optimizer,metrics = Metrics)\n",
        "model3.compile(loss = loss,optimizer = optimizer,metrics = Metrics)"
      ],
      "metadata": {
        "id": "Mm5tpVADbn9Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs =10\n",
        "validation_Set = (x_valid,y_valid)\n",
        "history = model.fit(x_train,y_train,epochs = epochs,validation_data= validation_Set,batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBGnfA3wcHce",
        "outputId": "0997ba45-ee22-4b99-979c-22307461b42f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.5676 - accuracy: 0.8507 - val_loss: 0.2946 - val_accuracy: 0.9166\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2805 - accuracy: 0.9194 - val_loss: 0.2323 - val_accuracy: 0.9330\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2285 - accuracy: 0.9355 - val_loss: 0.1990 - val_accuracy: 0.9442\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1931 - accuracy: 0.9449 - val_loss: 0.1728 - val_accuracy: 0.9522\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1678 - accuracy: 0.9518 - val_loss: 0.1591 - val_accuracy: 0.9570\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1479 - accuracy: 0.9572 - val_loss: 0.1431 - val_accuracy: 0.9606\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1323 - accuracy: 0.9618 - val_loss: 0.1338 - val_accuracy: 0.9644\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1190 - accuracy: 0.9658 - val_loss: 0.1198 - val_accuracy: 0.9670\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1080 - accuracy: 0.9690 - val_loss: 0.1184 - val_accuracy: 0.9662\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0990 - accuracy: 0.9719 - val_loss: 0.1103 - val_accuracy: 0.9662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model2.fit(x_train,y_train,epochs = epochs,validation_data= validation_Set,batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R51Uk9A8cVQs",
        "outputId": "b1ffd74b-64df-488b-dda8-e2c9f260f0b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.5961 - accuracy: 0.8465 - val_loss: 0.3059 - val_accuracy: 0.9174\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2867 - accuracy: 0.9187 - val_loss: 0.2417 - val_accuracy: 0.9326\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2350 - accuracy: 0.9331 - val_loss: 0.2035 - val_accuracy: 0.9434\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2017 - accuracy: 0.9422 - val_loss: 0.1880 - val_accuracy: 0.9488\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1766 - accuracy: 0.9496 - val_loss: 0.1619 - val_accuracy: 0.9554\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1571 - accuracy: 0.9545 - val_loss: 0.1471 - val_accuracy: 0.9598\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1417 - accuracy: 0.9595 - val_loss: 0.1418 - val_accuracy: 0.9616\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1285 - accuracy: 0.9635 - val_loss: 0.1272 - val_accuracy: 0.9650\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1173 - accuracy: 0.9671 - val_loss: 0.1197 - val_accuracy: 0.9680\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1072 - accuracy: 0.9696 - val_loss: 0.1130 - val_accuracy: 0.9682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history3 = model3.fit(x_train,y_train,epochs = epochs,validation_data= validation_Set,batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgcgk1_ZcdtG",
        "outputId": "d7fa87f2-d86e-4c03-c999-031ac294e1ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 10s 5ms/step - loss: 0.6162 - accuracy: 0.8408 - val_loss: 0.3087 - val_accuracy: 0.9150\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2916 - accuracy: 0.9170 - val_loss: 0.2438 - val_accuracy: 0.9292\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2388 - accuracy: 0.9324 - val_loss: 0.2056 - val_accuracy: 0.9430\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2042 - accuracy: 0.9415 - val_loss: 0.1778 - val_accuracy: 0.9526\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.1780 - accuracy: 0.9486 - val_loss: 0.1619 - val_accuracy: 0.9556\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1576 - accuracy: 0.9554 - val_loss: 0.1428 - val_accuracy: 0.9618\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1408 - accuracy: 0.9606 - val_loss: 0.1321 - val_accuracy: 0.9658\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1274 - accuracy: 0.9641 - val_loss: 0.1200 - val_accuracy: 0.9682\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1156 - accuracy: 0.9668 - val_loss: 0.1161 - val_accuracy: 0.9686\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1057 - accuracy: 0.9702 - val_loss: 0.1073 - val_accuracy: 0.9698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9uYJEQJciS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}